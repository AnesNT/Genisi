import os
import time
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS

app = Flask(__name__)
CORS(app)

# =======================================================
# ğŸ” Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª: Llama 3.1 Engine
# =======================================================
# Ø§Ù„Ù…ÙØªØ§Ø­ ÙŠØªÙ… Ø¬Ù„Ø¨Ù‡ Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø¨ÙŠØ¦Ø© Render
# ØªØ£ÙƒØ¯ Ø£Ù†Ùƒ ÙˆØ¶Ø¹Øª Ù…ØªØºÙŠØ± Ø§Ø³Ù…Ù‡ HF_KEY ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Render
HF_TOKEN = os.environ.get("HF_KEY") 

# Ø±Ø§Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø³Ù…ÙŠ ÙÙŠ Hugging Face
API_URL = "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct?inference_provider=sambanova"

# Ø¯Ø§Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Llama
def query_llama(prompt, retries=3):
    headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    
    # 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ Llama 3
    # Llama 3 ÙŠÙÙ‡Ù… Ù‡ÙŠÙƒÙ„ÙŠØ© Ø®Ø§ØµØ© Ù„Ù„Ø£ÙˆØ§Ù…Ø± (System > User > Assistant)
    full_prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are Genisi, an advanced AI developed by AnesNT.
Your goal is to provide helpful, accurate, and concise answers.
You speak the user's language fluently (Arabic/English/etc).
<|eot_id|><|start_header_id|>user<|end_header_id|>
{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""

    payload = {
        "inputs": full_prompt,
        "parameters": {
            "max_new_tokens": 1024,  # Ø·ÙˆÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            "temperature": 0.7,      # Ù†Ø³Ø¨Ø© Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹
            "top_p": 0.9,
            "return_full_text": False
        }
    }

    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ø¹ ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© ÙÙŠ Ø­Ø§Ù„Ø© "Ø§Ù„Ø³ÙŠØ±ÙØ± Ù…Ø´ØºÙˆÙ„"
    for i in range(retries):
        try:
            response = requests.post(API_URL, headers=headers, json=payload, timeout=30)
            
            # Ø­Ø§Ù„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ (ØªØ­Ø¯Ø« ÙÙŠ HF Free Tier)
            if response.status_code == 503:
                print(f"Model loading... wait {i+1}s")
                time.sleep(2)
                continue # Ù†Ø¹ÙŠØ¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
                
            if response.status_code != 200:
                raise Exception(f"HF Error {response.status_code}: {response.text}")

            result = response.json()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ
            if isinstance(result, list):
                return result[0]['generated_text']
            elif 'generated_text' in result:
                return result['generated_text']
            else:
                return str(result)
                
        except Exception as e:
            print(f"Attempt {i+1} failed: {e}")
            if i == retries - 1: # Ø¢Ø®Ø± Ù…Ø­Ø§ÙˆÙ„Ø©
                raise e

    return "Llama server is busy right now. Please try again."

@app.route('/chat', methods=['POST'])
def chat():
    data = request.json
    text = data.get('text', '')

    if not text:
        return jsonify({"type": "error", "reply": "Empty message"})

    try:
        # ÙØ­Øµ Ø¨Ø³ÙŠØ· Ù„Ù„ØµÙˆØ± (Ø³Ù†Ø¨Ù‚ÙŠ Ø§Ù„Ù…ÙŠØ²Ø© ÙÙ‡ÙŠ Ù„Ø§ Ø¹Ù„Ø§Ù‚Ø© Ù„Ù‡Ø§ Ø¨Ø¬ÙˆØ¬Ù„)
        if any(x in text.lower() for x in ['image', 'draw', 'Ø±Ø³Ù…', 'ØµÙˆØ±Ø©', 'ØªØ®ÙŠÙ„']):
            return jsonify({
                "type": "image", 
                "reply": "Flux Generator"
            })

        # Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Llama
        reply = query_llama(text)
        return jsonify({"type": "text", "reply": reply.strip()})

    except Exception as e:
        return jsonify({"type": "error", "reply": str(e)}), 500

if __name__ == '__main__':
    port = int(os.environ.get("PORT", 5000))
    app.run(host='0.0.0.0', port=port)
